Experiment: base_train
Logger directory: logs/base_train
2023-10-24 10:31:03 ==> Namespace(folder='/kaggle/input/shapenet', json='final.json', b_tag='depth', log_dir='logs', exp='base_train', batch_size=24, size=256, latent_dim=1024, epoch=0, scheduler='step', gamma=0.5, n_epochs=30, decay_epoch=10, save_iter=100, lr=0.0001, gpu=0, modelPath=None, pcn='weights/pcn_base/model.pth', test=False, testSave=False, resume=False, lambda_pixel=10, lambda_latent=0.5, lambda_kl=0.01)
2023-10-24 10:31:03 ==> ------------------Epoch: 0------------------
2023-10-24 11:38:55 ==> Epoch 0 Train Loss: 65.70007925387472
2023-10-24 11:47:31 ==> Epoch 0 Val Loss: 40.71485057473183 Learning Rate: 5e-05
2023-10-24 11:47:32 ==> Epoch 0 Best Model Saved
2023-10-24 11:47:32 ==> Last Model saved (best loss 40.7149 at epoch 0)
2023-10-24 11:47:32 ==> ------------------Epoch: 1------------------
2023-10-24 12:51:00 ==> Epoch 1 Train Loss: 57.58647069800645
2023-10-24 12:57:45 ==> Epoch 1 Val Loss: 38.851092448458076 Learning Rate: 2.5e-05
2023-10-24 12:57:46 ==> Epoch 1 Best Model Saved
2023-10-24 12:57:46 ==> Last Model saved (best loss 38.8511 at epoch 1)
2023-10-24 12:57:46 ==> ------------------Epoch: 2------------------
2023-10-24 13:58:09 ==> Epoch 2 Train Loss: 55.40630198200233
2023-10-24 14:05:02 ==> Epoch 2 Val Loss: 53.14447284210473 Learning Rate: 1.25e-05
2023-10-24 14:05:02 ==> Last Model saved (best loss 38.8511 at epoch 1)
2023-10-24 14:05:02 ==> ------------------Epoch: 3------------------
2023-10-24 15:04:24 ==> Epoch 3 Train Loss: 54.326120940968394
2023-10-24 15:11:06 ==> Epoch 3 Val Loss: 35.73006298625842 Learning Rate: 6.25e-06
2023-10-24 15:11:06 ==> Epoch 3 Best Model Saved
2023-10-24 15:11:06 ==> Last Model saved (best loss 35.7301 at epoch 3)
2023-10-24 15:11:06 ==> ------------------Epoch: 4------------------
2023-10-24 16:11:29 ==> Epoch 4 Train Loss: 53.802360511617735
2023-10-24 16:17:58 ==> Epoch 4 Val Loss: 717.6302497601137 Learning Rate: 3.125e-06
2023-10-24 16:17:59 ==> Last Model saved (best loss 35.7301 at epoch 3)
2023-10-24 16:17:59 ==> ------------------Epoch: 5------------------
2023-10-24 17:17:24 ==> Epoch 5 Train Loss: 53.52983511518687
2023-10-24 17:23:49 ==> Epoch 5 Val Loss: 35.8546913578175 Learning Rate: 1.5625e-06
2023-10-24 17:23:49 ==> Last Model saved (best loss 35.7301 at epoch 3)
2023-10-24 17:23:49 ==> ------------------Epoch: 6------------------
2023-10-24 18:27:40 ==> Epoch 6 Train Loss: 53.57931426377036
2023-10-24 18:34:49 ==> Epoch 6 Val Loss: 35.30104213277809 Learning Rate: 7.8125e-07
2023-10-24 18:34:49 ==> Epoch 6 Best Model Saved
2023-10-24 18:34:50 ==> Last Model saved (best loss 35.3010 at epoch 6)
2023-10-24 18:34:50 ==> ------------------Epoch: 7------------------
2023-10-24 19:39:00 ==> Epoch 7 Train Loss: 53.39643499464728
2023-10-24 19:46:01 ==> Epoch 7 Val Loss: 228.29337621107697 Learning Rate: 3.90625e-07
2023-10-24 19:46:01 ==> Last Model saved (best loss 35.3010 at epoch 6)
2023-10-24 19:46:01 ==> ------------------Epoch: 8------------------
2023-10-24 20:50:42 ==> Epoch 8 Train Loss: 53.41700586723164
2023-10-24 20:57:47 ==> Epoch 8 Val Loss: 83.82922058459371 Learning Rate: 1.953125e-07
2023-10-24 20:57:48 ==> Last Model saved (best loss 35.3010 at epoch 6)
2023-10-24 20:57:48 ==> ------------------Epoch: 9------------------
2023-10-24 22:02:19 ==> Epoch 9 Train Loss: 53.450871135573834
2023-10-24 22:09:17 ==> Epoch 9 Val Loss: 35.15674098976888 Learning Rate: 9.765625e-08
2023-10-24 22:09:17 ==> Epoch 9 Best Model Saved
2023-10-24 22:09:18 ==> Last Model saved (best loss 35.1567 at epoch 9)
2023-10-24 22:09:18 ==> ------------------Epoch: 10------------------

Experiment: base
Logger directory: logs/base
2023-10-25 07:06:43 ==> Namespace(folder='/kaggle/input/shapenet', json='final.json', b_tag='depth', log_dir='logs', exp='base', batch_size=24, size=256, latent_dim=1024, epoch=0, scheduler='step', gamma=0.8, n_epochs=18, decay_epoch=10, save_iter=1000, lr=1e-05, gpu=0, modelPath='weights/base/model.pth', pcn='weights/pcn_base/model.pth', test=False, testSave=False, resume=True, lambda_coarse=0.2, lambda_pixel=10, lambda_latent=0.8, lambda_kl=0.01)
2023-10-25 07:06:43 ==> Loading checkpoint from weights/base/model.pth
2023-10-25 07:06:43 ==> Checkpoint loaded (epoch 9, loss 35.15674098976888)
2023-10-25 07:06:43 ==> ------------------Epoch: 10------------------
2023-10-25 08:16:13 ==> Epoch 10 Train Loss: 25.100733570870943
2023-10-25 08:23:56 ==> Epoch 10 Val Loss: 48.651333732996136 Learning Rate: 8.000000000000001e-06
2023-10-25 08:23:56 ==> Epoch 10 Best Model Saved
2023-10-25 08:23:57 ==> Last Model saved (best loss 48.6513 at epoch 10)
2023-10-25 08:23:57 ==> ------------------Epoch: 11------------------
2023-10-25 09:20:52 ==> Epoch 11 Train Loss: 25.033646963420324
2023-10-25 09:27:07 ==> Epoch 11 Val Loss: 40.500898328609765 Learning Rate: 6.400000000000001e-06
2023-10-25 09:27:07 ==> Epoch 11 Best Model Saved
2023-10-25 09:27:07 ==> Last Model saved (best loss 40.5009 at epoch 11)
2023-10-25 09:27:07 ==> ------------------Epoch: 12------------------
2023-10-25 10:24:00 ==> Epoch 12 Train Loss: 24.92922441160772
2023-10-25 10:30:24 ==> Epoch 12 Val Loss: 39.52495661331341 Learning Rate: 5.120000000000002e-06
2023-10-25 10:30:25 ==> Epoch 12 Best Model Saved
2023-10-25 10:30:25 ==> Last Model saved (best loss 39.5250 at epoch 12)
2023-10-25 10:30:25 ==> ------------------Epoch: 13------------------
2023-10-25 11:28:10 ==> Epoch 13 Train Loss: 24.91629396507051
2023-10-25 11:34:35 ==> Epoch 13 Val Loss: 134.42220981465653 Learning Rate: 4.096000000000002e-06
2023-10-25 11:34:35 ==> Last Model saved (best loss 39.5250 at epoch 12)
2023-10-25 11:34:35 ==> ------------------Epoch: 14------------------
2023-10-25 12:33:04 ==> Epoch 14 Train Loss: 24.853363929432817
2023-10-25 12:39:28 ==> Epoch 14 Val Loss: 109.03162331553176 Learning Rate: 3.276800000000002e-06
2023-10-25 12:39:28 ==> Last Model saved (best loss 39.5250 at epoch 12)
2023-10-25 12:39:28 ==> ------------------Epoch: 15------------------
2023-10-25 13:36:29 ==> Epoch 15 Train Loss: 24.828800475224853
2023-10-25 13:42:46 ==> Epoch 15 Val Loss: 40.64447140786797 Learning Rate: 2.6214400000000015e-06
2023-10-25 13:42:47 ==> Last Model saved (best loss 39.5250 at epoch 12)
2023-10-25 13:42:47 ==> ------------------Epoch: 16------------------
2023-10-25 14:39:58 ==> Epoch 16 Train Loss: 24.793048998108134
2023-10-25 14:46:16 ==> Epoch 16 Val Loss: 39.06035712803714 Learning Rate: 2.0971520000000012e-06
2023-10-25 14:46:17 ==> Epoch 16 Best Model Saved
2023-10-25 14:46:17 ==> Last Model saved (best loss 39.0604 at epoch 16)
2023-10-25 14:46:17 ==> ------------------Epoch: 17------------------
2023-10-25 15:43:02 ==> Epoch 17 Train Loss: 24.68694888812024
2023-10-25 15:49:19 ==> Epoch 17 Val Loss: 38.98785361088812 Learning Rate: 1.6777216000000011e-06
2023-10-25 15:49:20 ==> Epoch 17 Best Model Saved
2023-10-25 15:49:20 ==> Last Model saved (best loss 38.9879 at epoch 17)
Experiment: base
Logger directory: logs/base
2023-10-25 17:42:00 ==> Namespace(folder='/kaggle/input/shapenet', json='final.json', b_tag='depth', log_dir='logs', exp='base', batch_size=24, size=256, latent_dim=1024, epoch=0, scheduler='step', gamma=0.8, n_epochs=29, decay_epoch=10, save_iter=1000, lr=1e-05, gpu=0, modelPath='weights/base/model.pth', pcn='weights/pcn_base/model.pth', test=False, testSave=False, resume=True, lambda_coarse=0.05, lambda_chamfer=2, lambda_pixel=10, lambda_latent=1.0, lambda_kl=0.01)
2023-10-25 17:42:00 ==> Loading checkpoint from weights/base/model.pth
2023-10-25 17:42:01 ==> Checkpoint loaded (epoch 17, loss 38.98785361088812)
2023-10-25 17:42:01 ==> ------------------Epoch: 18------------------
2023-10-25 18:52:20 ==> Epoch 18 Train Loss: 23.58278093044646
2023-10-25 19:00:16 ==> Epoch 18 Val Loss: 27.20664708642289 Learning Rate: 8.000000000000001e-06
2023-10-25 19:00:16 ==> Epoch 18 Best Model Saved
2023-10-25 19:00:17 ==> Last Model saved (best loss 27.2066 at epoch 18)
2023-10-25 19:00:17 ==> ------------------Epoch: 19------------------
2023-10-25 19:57:30 ==> Epoch 19 Train Loss: 23.396934549382422
2023-10-25 20:03:48 ==> Epoch 19 Val Loss: 27.348731975071132 Learning Rate: 6.400000000000001e-06
2023-10-25 20:03:48 ==> Last Model saved (best loss 27.2066 at epoch 18)
2023-10-25 20:03:48 ==> ------------------Epoch: 20------------------
2023-10-25 21:00:46 ==> Epoch 20 Train Loss: 23.390484624833334
2023-10-25 21:07:03 ==> Epoch 20 Val Loss: 29.523589240852743 Learning Rate: 5.120000000000002e-06
2023-10-25 21:07:04 ==> Last Model saved (best loss 27.2066 at epoch 18)
2023-10-25 21:07:04 ==> ------------------Epoch: 21------------------
2023-10-25 22:03:56 ==> Epoch 21 Train Loss: 23.25040048424853
2023-10-25 22:10:12 ==> Epoch 21 Val Loss: 26.5799223841168 Learning Rate: 4.096000000000002e-06
2023-10-25 22:10:12 ==> Epoch 21 Best Model Saved
2023-10-25 22:10:13 ==> Last Model saved (best loss 26.5799 at epoch 21)
2023-10-25 22:10:13 ==> ------------------Epoch: 22------------------
2023-10-25 23:07:04 ==> Epoch 22 Train Loss: 23.209861067589372
2023-10-25 23:13:22 ==> Epoch 22 Val Loss: 26.51498571736738 Learning Rate: 3.276800000000002e-06
2023-10-25 23:13:22 ==> Epoch 22 Best Model Saved
2023-10-25 23:13:23 ==> Last Model saved (best loss 26.5150 at epoch 22)
2023-10-25 23:13:23 ==> ------------------Epoch: 23------------------
2023-10-26 00:10:19 ==> Epoch 23 Train Loss: 23.11597491730936
2023-10-26 00:16:38 ==> Epoch 23 Val Loss: 26.5154708805494 Learning Rate: 2.6214400000000015e-06
2023-10-26 00:16:38 ==> Last Model saved (best loss 26.5150 at epoch 22)
2023-10-26 00:16:38 ==> ------------------Epoch: 24------------------
2023-10-26 01:13:29 ==> Epoch 24 Train Loss: 23.108706178318243
2023-10-26 01:19:48 ==> Epoch 24 Val Loss: 26.341848771553487 Learning Rate: 2.0971520000000012e-06
2023-10-26 01:19:48 ==> Epoch 24 Best Model Saved
2023-10-26 01:19:49 ==> Last Model saved (best loss 26.3418 at epoch 24)
2023-10-26 01:19:49 ==> ------------------Epoch: 25------------------
2023-10-26 02:16:58 ==> Epoch 25 Train Loss: 23.05503539100755
2023-10-26 02:23:18 ==> Epoch 25 Val Loss: 26.061184885911644 Learning Rate: 1.6777216000000011e-06
2023-10-26 02:23:19 ==> Epoch 25 Best Model Saved
2023-10-26 02:23:19 ==> Last Model saved (best loss 26.0612 at epoch 25)
2023-10-26 02:23:19 ==> ------------------Epoch: 26------------------
2023-10-26 03:20:26 ==> Epoch 26 Train Loss: 23.066428743186407
2023-10-26 03:26:43 ==> Epoch 26 Val Loss: 26.40773199731484 Learning Rate: 1.342177280000001e-06
2023-10-26 03:26:43 ==> Last Model saved (best loss 26.0612 at epoch 25)
2023-10-26 03:26:43 ==> ------------------Epoch: 27------------------
2023-10-26 04:24:15 ==> Epoch 27 Train Loss: 23.051400046097115
2023-10-26 04:30:36 ==> Epoch 27 Val Loss: 26.81258838158101 Learning Rate: 1.073741824000001e-06
2023-10-26 04:30:36 ==> Last Model saved (best loss 26.0612 at epoch 25)
2023-10-26 04:30:36 ==> ------------------Epoch: 28------------------
2023-10-26 05:28:16 ==> Epoch 28 Train Loss: 22.981221067602746
2023-10-26 05:34:36 ==> Epoch 28 Val Loss: 26.219556530704722 Learning Rate: 8.589934592000008e-07
2023-10-26 05:34:37 ==> Last Model saved (best loss 26.0612 at epoch 25)